# SVM-vs-ANN
An experiment comparing the effectiveness of SVM and ANN techniques in simple OCR. This project was done for my Applied Mathematical Seminar. I used Pytorch and Scikit-learn to implement an SVM and ANN in Python on a simple OCR dataset. The characters were already broken down into simple feature vectors. I am very confident in my approach on the SVM as I tested various kernels, costs and gamma values. I do think there were some missing kernels in Scikit-learn compared to what I saw in R's KSVM module, but I may have missed them.

The ANN was a far more complex model and one that I believe has room for significant improvement. ANNs can of course be constructed into a CNN which would likely outperform an SVM but I believe this would only be applicable when working with the raw image data. Likewise, a RNN would likely outperform if trying to recognize words or sentences as it takes sequence into account. This project makes use of a simple FNN which performs somewhat worse than the SVM. There are likely different architectures that could have been implemented that would have performed better. I did not have time to learn how to automate metaparameter tuning for this project but it would likely have created a far better ANN.

While the ANN did perform worse than the SVM in overall accuracy and ease to construct a model, there are definite benefits to an ANN. For one, you can see the probabilities for each possible character. The correct character will usually be the 2nd or 3rd most likely if the first is incorrect. This can give hints for optimization and also be useful in an applied setting if exposed to a user. I believe one way that the ANN could definitely have been improved would be by a manual selection of feature vectors. Additional vectors like extracted from various transformations could have extracted less obvious features than the manual pixel densities in the dataset I used.
